{
  "timestamp": "2025-09-07T09:29:28.288738",
  "tools_analyzed": {
    "langchain_tools": {
      "tool_name": "langchain_tools",
      "file_count": 145,
      "function_definitions": [
        "multiply",
        "add",
        "addition",
        "prompt",
        "failing_dynamic_model",
        "prompt_no_store",
        "dynamic_model",
        "_run",
        "node_update_parent_tool",
        "_make_tool",
        "concurrent_runs",
        "async_transfer_to_bob",
        "multiplication",
        "my_func",
        "transfer_to_bob",
        "pre_model_hook",
        "__init__",
        "custom_condition",
        "remove_all_messages_tool",
        "execute_agent"
      ],
      "class_definitions": [
        "ToolOutputMixin",
        "ModelB",
        "JokeDict",
        "fooSchema",
        "JsonOutputToolsParser",
        "ToolCalls",
        "FakeChatModelStartTracer",
        "Address",
        "TrajectoryEvalChain",
        "AgentStateWithStructuredResponsePydantic",
        "AnswerWithJustification",
        "TestResponseFormatAsProviderStrategy",
        "InjectedToolWithSchema",
        "Agent",
        "StreamingRunnable",
        "CustomStreamEvent",
        "WeatherResponse",
        "ToolCall",
        "FooSchema",
        "TestAzureOpenAIStandardLegacy"
      ],
      "import_dependencies": [
        "ValidationError as ValidationErrorV1",
        "ToolOutputMixin",
        "CallbackManagerForLLMRun",
        "RunnableConfig, RunnableSerializable, ensure_config",
        "Document",
        "_any_id_ai_message, _any_id_ai_message_chunk",
        "TEXTWORLD_PROMPT",
        "ValidationError",
        "Any, AsyncIterator, Iterator",
        "Any, Callable, Literal, Optional, Union, cast",
        "Exa",
        "from_env, secret_from_env",
        "AgentType, initialize_agent",
        "sleep",
        "UUID",
        "create_model_v2",
        "as_import_path",
        "ConfigDict, model_validator",
        "TrajectoryEvalChain",
        "EventSource, aconnect_sse, connect_sse"
      ],
      "function_calling_patterns": [
        "function_calling\", include_raw=True:\n\n            .. code-block:: python\n\n                from langchain_fireworks import ChatFireworks\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel)",
        "function_call\": {\n                \"name\": \"function_name\",\n                \"arguments\": '{\"arg1\": \"code\\ncode\"}',\n            }\n        },\n    )\n    chat_generation = ChatGeneration(message=message)",
        "function_call,\n                        \"arguments\": json.loads(\n                            function_call[\"arguments\"], strict=self.strict\n                        )",
        "function_call\":\n                    for fkey, fvalue in value.items()",
        "function_call(invoke: Any, tools: list[dict])",
        "function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool,\n)",
        "function_calling\", include_raw=False:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)",
        "function_call`, you must provide exactly one \"\n                    \"function.\"\n                )\n                raise ValueError(msg)",
        "function_call and function_call[\"name\"] is None:\n            function_call[\"name\"] = \"\"\n        additional_kwargs[\"function_call\"] = function_call\n    if raw_tool_calls := _dict.get(\"tool_calls\")",
        "function_call\",\n                id=\"func_123\",\n                call_id=\"call_123\",\n                name=\"get_weather\",\n                arguments='{\"location\": \"New York\"}',\n            ),\n        ],\n        metadata=dict(key1=\"value1\", key2=\"value2\")",
        "function_calling import (\n    _convert_typed_dict_to_openai_function,\n    convert_to_json_schema,\n    convert_to_openai_function,\n    tool_example_to_messages,\n)",
        "function_call and function_call[\"name\"] is None:\n            function_call[\"name\"] = \"\"\n        additional_kwargs[\"function_call\"] = function_call\n    tool_call_chunks = []\n    if raw_tool_calls := _dict.get(\"tool_calls\")",
        "function_call\"\n    ):\n        _advance(chunk.output_index)",
        "function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n                .. versionchanged:: 0.1.9\n\n                    Added support for TypedDict class.\n\n            method:\n                The method for steering model generation, either ``'function_calling'``\n                or ``'json_mode'``. If ``'function_calling'`` then the schema will be converted\n                to an OpenAI function and the returned model will make use of the\n                function-calling API. If ``'json_mode'`` then OpenAI's JSON mode will be\n                used.\n\n                .. note::\n                    If using ``'json_mode'`` then you must include instructions for formatting\n                    the output into the desired schema into the model call. (either via the\n                    prompt itself or in the system message/prompt/instructions)",
        "function_call_ids = message.additional_kwargs.get(_FUNCTION_CALL_IDS_MAP_KEY)",
        "function_calling\", include_raw=True:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel)",
        "function_call\": {\"name\": \"function_name\", \"arguments\": {}}\n            },\n        ),\n        # Bad function call information (arguments should be proper json)",
        "function_callback(\n            f\"{get_colored_text('[llm/error]', color='red')",
        "function_calling import (\n    PYTHON_TO_JSON_TYPES,\n    convert_to_openai_function,\n)",
        "function_calling\"},\n                    \"schema\": formatted_tool,\n                },\n            )\n            if is_pydantic_schema:\n                output_parser: OutputParserLike = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "openai_function_calling": {
      "tool_name": "openai_function_calling",
      "file_count": 107,
      "function_definitions": [],
      "class_definitions": [
        "GetWeatherArgs",
        "Message",
        "OrderBy",
        "RealtimeConversationItemFunctionCallOutputParam",
        "RequiredAction",
        "TestThreads",
        "DataSourceCreateEvalResponsesRunDataSourceSourceResponses",
        "Step",
        "ResponseCustomToolCallInputDeltaEvent",
        "ToolOutput",
        "ThreadToolResourcesFileSearchVectorStoreChunkingStrategyAuto",
        "StepListParams",
        "FunctionCallOutput",
        "DataSourceResponsesSamplingParamsText",
        "CompletionsWithRawResponse",
        "TestAsyncRuns",
        "AsyncRealtimeInputAudioBufferResource",
        "TruncationStrategy",
        "RealtimeConversationResource",
        "ResultCounts"
      ],
      "import_dependencies": [
        "ResponseMcpCallInProgressEvent",
        "Message",
        "ConversationItemCreateEventParam as ConversationItemCreateEventParam",
        "RealtimeConversationItemFunctionCallOutputParam",
        "RunStepDelta as RunStepDelta",
        "TYPE_CHECKING, Any, Iterable, cast",
        "ErrorEvent as ErrorEvent",
        "ResponseImageGenCallCompletedEvent",
        "ToolCallDelta",
        "ChatCompletionAudio as ChatCompletionAudio",
        "ResponseOutputMessageParam",
        "RealtimeTracingConfigParam as RealtimeTracingConfigParam",
        "RealtimeMcpToolExecutionError as RealtimeMcpToolExecutionError",
        "CustomTool as CustomTool",
        "List, Optional",
        "ResponseTextDoneEvent as ResponseTextDoneEvent",
        "ResponseComputerToolCallParam",
        "EvalAPIError",
        "AsyncIterator",
        "RealtimeSessionCreateResponse as RealtimeSessionCreateResponse"
      ],
      "function_calling_patterns": [
        "function_call=None,\n        parsed=ColorDetection(color=<Color.RED: 'red'>, hex_color_code='#FF0000')",
        "function_call` | `function_call_output`) this\n    field allows the client to assign the unique ID of the item. It is not required\n    because the server will generate one if not provided.\n\n    For an item of type `item_reference`, this field is required and is a reference\n    to any item that has previously existed in the conversation.\n    \"\"\"\n\n    arguments: Optional[str] = None\n    \"\"\"The arguments of the function call (for `function_call` items)",
        "function_call = response.output[0]\nassert function_call.type == \"function_call\"\nassert isinstance(function_call.parsed_arguments, Query)",
        "function_call=None,\n            parsed=None,\n            refusal=None,\n            role='assistant',\n            tool_calls=None\n        )\n    )\n]\n\"\"\"\n    )\n\n\n@pytest.mark.parametrize(\"sync\", [True, False], ids=[\"sync\", \"async\"])",
        "function_call import (\n    RealtimeConversationItemFunctionCall as RealtimeConversationItemFunctionCall,\n)",
        "function_call.name)\n    if not input_tool:\n        return None\n\n    tool = cast(object, input_tool)",
        "function_call=None,\n                parsed=Location(city='San Francisco', temperature=58.0, units='f')",
        "function_call\":\n                snapshot.output.append(\n                    construct_type_unchecked(\n                        type_=cast(Any, ParsedResponseFunctionToolCall)",
        "function_call=function_call,\n            functions=functions,\n            logit_bias=logit_bias,\n            logprobs=logprobs,\n            max_completion_tokens=max_completion_tokens,\n            max_tokens=max_tokens,\n            metadata=metadata,\n            modalities=modalities,\n            n=n,\n            parallel_tool_calls=parallel_tool_calls,\n            prediction=prediction,\n            presence_penalty=presence_penalty,\n            prompt_cache_key=prompt_cache_key,\n            reasoning_effort=reasoning_effort,\n            safety_identifier=safety_identifier,\n            seed=seed,\n            service_tier=service_tier,\n            store=store,\n            stop=stop,\n            stream_options=stream_options,\n            temperature=temperature,\n            tool_choice=tool_choice,\n            tools=tools,\n            top_logprobs=top_logprobs,\n            top_p=top_p,\n            user=user,\n            verbosity=verbosity,\n            web_search_options=web_search_options,\n            extra_headers=extra_headers,\n            extra_query=extra_query,\n            extra_body=extra_body,\n            timeout=timeout,\n        )\n        return ChatCompletionStreamManager(\n            api_request,\n            response_format=response_format,\n            input_tools=tools,\n        )",
        "function_call_param import (\n    RealtimeConversationItemFunctionCallParam as RealtimeConversationItemFunctionCallParam,\n)",
        "function_call_output` items).\"\"\"\n\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    \"\"\"\n    The role of the message sender (`user`, `assistant`, `system`)",
        "function_call=None,\n            parsed=Location(city='San Francisco', temperature=59.0, units='f')",
        "function_call_output\"]\n    \"\"\"The type of the function tool call output. Always `function_call_output`.\"\"\"\n\n    id: Optional[str] = None\n    \"\"\"The unique ID of the function tool call output.\n\n    Populated when this item is returned via API.\n    \"\"\"\n\n    status: Optional[Literal[\"in_progress\", \"completed\", \"incomplete\"]] = None\n    \"\"\"The status of the item.\n\n    One of `in_progress`, `completed`, or `incomplete`. Populated when items are\n    returned via API.\n    \"\"\"\n\n\nclass ImageGenerationCall(BaseModel)",
        "function_call` items).\"\"\"\n\n    call_id: str\n    \"\"\"\n    The ID of the function call (for `function_call` and `function_call_output`\n    items)",
        "function_call_output` item, the server will check that a\n    `function_call` item with the same ID exists in the conversation history.\n    \"\"\"\n\n    content: Optional[List[ConversationItemContent]] = None\n    \"\"\"The content of the message, applicable for `message` items.\n\n    - Message items of role `system` support only `input_text` content\n    - Message items of role `user` support `input_text` and `input_audio` content\n    - Message items of role `assistant` support `text` content.\n    \"\"\"\n\n    name: Optional[str] = None\n    \"\"\"The name of the function being called (for `function_call` items)",
        "function_call_arguments_done_event import ResponseFunctionCallArgumentsDoneEvent\nfrom .response_function_call_arguments_delta_event import ResponseFunctionCallArgumentsDeltaEvent\nfrom .conversation_item_input_audio_transcription_segment import ConversationItemInputAudioTranscriptionSegment\nfrom .conversation_item_input_audio_transcription_delta_event import ConversationItemInputAudioTranscriptionDeltaEvent\nfrom .conversation_item_input_audio_transcription_failed_event import ConversationItemInputAudioTranscriptionFailedEvent\nfrom .conversation_item_input_audio_transcription_completed_event import (\n    ConversationItemInputAudioTranscriptionCompletedEvent,\n)",
        "function_call_output_param import (\n    RealtimeConversationItemFunctionCallOutputParam as RealtimeConversationItemFunctionCallOutputParam,\n)",
        "function_call\", \"function_call_output\", \"item_reference\"]] = None\n    \"\"\"\n    The type of the item (`message`, `function_call`, `function_call_output`,\n    `item_reference`)",
        "function_call\"]\n    \"\"\"The reason the model stopped generating tokens.\n\n    This will be `stop` if the model hit a natural stop point or a provided stop\n    sequence, `length` if the maximum number of tokens specified in the request was\n    reached, `content_filter` if content was omitted due to a flag from our content\n    filters, `tool_calls` if the model called a tool, or `function_call`\n    (deprecated)",
        "function_call=None,\n            parsed=None,\n            refusal=None,\n            role='assistant',\n            tool_calls=None\n        )\n    )\n]\n\"\"\")\n\n\n@pytest.mark.respx(base_url=base_url)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "autogen_tools": {
      "tool_name": "autogen_tools",
      "file_count": 94,
      "function_definitions": [
        "get_user_input",
        "mock_server_params",
        "test_list_tools_operation",
        "test_mcp_client_list_tools_operation",
        "test_websocket_bridge_send_message",
        "mock_client_session",
        "test_mcp_client_error_handling",
        "test_client_initialize",
        "create_default_lite_team",
        "code_interpreter_example",
        "test_websocket_connection_url_generation",
        "test_call_tool_missing_name",
        "mock_websocket",
        "example_with_configs",
        "run_main",
        "condition_func",
        "chat_completions_stream",
        "fn",
        "hello",
        "bing_example"
      ],
      "class_definitions": [
        "BaseAzureAISearchTool",
        "SystemMessage",
        "Message",
        "TestAssistantAgentErrorHandling",
        "_MockChatCompletion",
        "CodeInterpreterToolConfig",
        "TestAssistantAgentStateManagement",
        "ReasoningModelContext",
        "DisplayQuizArgs",
        "MultimodalWebSurfer",
        "SelectSpeakerEvent",
        "TestAssistantAgentInitialization",
        "LangChainToolAdapter",
        "DummyMsg",
        "LlamaCppChatCompletionClient",
        "SimpleAssistantAgent",
        "CalculatorArgs",
        "TestAssistantAgentAdvancedToolFeatures",
        "RoundRobinGroupChatManager",
        "TextCanvasMemory"
      ],
      "import_dependencies": [
        "CancellationToken",
        "DefaultAzureCredential",
        "ModelInfo, validate_model_info",
        "AsyncGenerator",
        "TextCanvasMemory",
        "MonkeyPatch",
        "pandas as pd",
        "List, Sequence, Tuple",
        "GlobalContextConfig as ContextConfig",
        "_model_info",
        "TextMessage, ToolCallExecutionEvent, ToolCallRequestEvent, ToolCallSummaryMessage",
        "BaseAgentEvent, BaseChatMessage, TextMessage",
        "List, Optional",
        "Component",
        "Any, AsyncGenerator, List, Optional, Union",
        "FunctionCall",
        "AzureAIAgent",
        "VectorizedQuery",
        "GlobalSearch",
        "EVENT_LOGGER_NAME, AgentId, CancellationToken, FunctionCall, SingleThreadedAgentRuntime"
      ],
      "function_calling_patterns": [
        "function_calls\": \"function_calls\",\n        \"end_turn\": \"stop\",\n        \"tool_calls\": \"function_calls\",\n    }\n\n    return KNOWN_STOP_MAPPINGS.get(stop_reason, \"unknown\")",
        "function_calls\",\n                content=[FunctionCall(id=\"1\", name=\"handoff_to_agent2\", arguments=json.dumps({})",
        "function_call_contents,\n                        finish_reason=FinishReason.TOOL_CALLS,\n                    )\n                else:\n                    # Plain assistant text\n                    chat_history.add_assistant_message(msg.content)",
        "function_calling\": False,\n                \"vision\": False,\n                \"json_output\": False,\n                \"family\": ModelFamily.GPT_4O,\n                \"structured_output\": False,\n            },\n        )\n\n        # Create agent with memory\n        agent = AssistantAgent(name=\"memory_test_agent\", model_client=model_client, memory=[memory])",
        "function_calling\": True,\n                \"vision\": False,\n                \"json_output\": False,\n                \"family\": ModelFamily.GPT_4O,\n                \"structured_output\": False,\n            },\n        )\n\n        agent = AssistantAgent(\n            name=\"test_agent\",\n            model_client=model_client,\n            handoffs=[\"other_agent\"],\n            max_tool_iterations=1,\n        )",
        "function_calling: Required[bool]\n    \"\"\"True if the model supports function calling, otherwise False.\"\"\"\n    json_output: Required[bool]\n    \"\"\"True if the model supports json output, otherwise False. Note: this is different to structured json.\"\"\"\n    family: Required[ModelFamily.ANY | str]\n    \"\"\"Model family should be one of the constants from :py:class:`ModelFamily` or a string representing an unknown model family.\"\"\"\n    structured_output: Required[bool]\n    \"\"\"True if the model supports structured output, otherwise False. This is different to json_output.\"\"\"\n    multiple_system_messages: Optional[bool]\n    \"\"\"True if the model supports multiple, non-consecutive system messages, otherwise False.\"\"\"\n\n\ndef validate_model_info(model_info: ModelInfo)",
        "function_calling\": False,\n        \"json_output\": False,\n        \"family\": \"test-model\",\n        \"structured_output\": False,\n    }\n    # Create a mock content with unknown type\n    unknown_content = MagicMock()",
        "function_calls\"\n    assert isinstance(result.content, list)",
        "function_calling\": False, \"vision\": False, \"family\": ModelFamily.GPT_4O}\n\n        mock_context = MagicMock()",
        "function_calling\": False,\n        \"json_output\": True,\n        \"family\": ModelFamily.CLAUDE_3_5_SONNET,\n        \"structured_output\": False,\n        \"multiple_system_messages\": False,\n    },\n}\n\n# Model token limits (context window size)",
        "function_calling\": False, \"vision\": False, \"family\": ModelFamily.GPT_4O}\n\n        # Mock create_stream method\n        async def mock_create_stream(*args: Any, **kwargs: Any)",
        "function_calling\": False,\n        \"json_output\": False,\n        \"family\": \"test-model\",\n        \"structured_output\": False,\n    }\n\n    actor = McpSessionActor(StdioServerParams(command=\"echo\", args=[\"test\"])",
        "function_calling\": True,\n                \"vision\": False,\n                \"json_output\": False,\n                \"family\": ModelFamily.GPT_4O,\n                \"structured_output\": False,\n            },\n        )\n\n        agent = AssistantAgent(\n            name=\"test_agent\",\n            model_client=model_client,\n            handoffs=[\"agent2\", \"agent3\"],\n        )",
        "function_calling\": True,\n                \"vision\": False,\n                \"json_output\": False,\n                \"family\": ModelFamily.GPT_4O,\n                \"structured_output\": False,\n            },\n        )\n\n        def error_formatter(call: FunctionCall, result: FunctionExecutionResult)",
        "function_calls\"\n    assert result.usage is not None\n\n\n@pytest.mark.asyncio\nasync def test_openai_tool_choice_validation_error_integration()",
        "function_calling\", \"json_output\", \"family\"]\n    for field in required_fields:\n        if field not in model_info:\n            raise ValueError(\n                f\"Missing required field '{field}' in ModelInfo. \"\n                \"Starting in v0.4.7, the required fields are enforced.\"\n            )",
        "function_calls\"\n\n\n@pytest.mark.asyncio\nasync def test_tool_choice_required_no_tools_error()",
        "function_call\", \"functions\", \"n\"])\nrequired_create_args: Set[str] = set([\"model\"])",
        "function_calling\": True,\n                        \"json_output\": True,\n                        \"vision\": True,\n                        \"structured_output\": True,\n                    },\n                )\n\n                # Call the model directly.\n                model_result = await model_client.create(\n                    messages=[UserMessage(content=\"What is the capital of France?\", source=\"User\")",
        "function_calling\": True,\n            \"vision\": False,\n            \"json_output\": False,\n            \"family\": ModelFamily.GPT_4,\n            \"structured_output\": False,\n        },\n    )\n\n    agent = AssistantAgent(\n        \"test_agent\",\n        model_client=mock_client,\n        model_client_stream=True,\n        reflect_on_tool_use=True,\n        tools=[_echo_function],\n    )"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "semantic_kernel_skills": {
      "tool_name": "semantic_kernel_skills",
      "file_count": 184,
      "function_definitions": [
        "test_openapi_create_lights",
        "test_it_renders_kernel_functions_arg_from_template",
        "test_add_fail_id",
        "test_cmc_from_element",
        "new_agent_name",
        "test_open_ai_assistant_agent_get_response",
        "invoke",
        "test_openai_text_prompt_execution_settings_validation_manual",
        "func_default",
        "test_add_function_from_prompt_different_values",
        "test_function_choice_behavior_auto",
        "test_scoped_event_from_kernel_process_creates_scoped_event",
        "mock_actor_context",
        "test_scmc",
        "test_init_empty_service_id",
        "test_mcp_plugin_session_initialized",
        "test_add_message_with_image",
        "test_fc_dump_json",
        "override_stream",
        "setup"
      ],
      "class_definitions": [
        "MockResponseModel",
        "MockPlugin",
        "OpenAITextPromptExecutionSettings",
        "PersonDetails",
        "AzureAIInferenceChatCompletion",
        "ChatMessageContent",
        "Step",
        "ClassTest",
        "HandoffResponseMessage",
        "ChatHistoryInCosmosDB",
        "AzureAIAgentUtils",
        "BedrockAgentBase",
        "MCPPluginBase",
        "DummyResult",
        "DataModelClass",
        "StringPlugin",
        "OpenAIRealtimeBase",
        "ErrorMockRunData",
        "CustomObjectWithList",
        "AgentThreadActions"
      ],
      "import_dependencies": [
        "Message",
        "BaseModel, ConfigDict, Field",
        "FunctionExecutionException, KernelPluginInvalidConfigurationError",
        "CancellationToken",
        "KernelServiceNotFoundError",
        "Text",
        "AsyncGenerator",
        "main as simple_chatbot_with_image",
        "BlockTypes",
        "ContentException",
        "CodeBlockSyntaxError",
        "Agent, AgentResponseItem, AgentThread",
        "AzureAIAgentUtils",
        "KernelParameterMetadata",
        "TruncationStrategy",
        "pandas as pd",
        "ValidationError",
        "CodeInterpreter, CodeInterpreterToolCall",
        "Mapping",
        "DeepResearchTool"
      ],
      "function_calling_patterns": [
        "function_call: FunctionCallContent):\n    # Test dumping the function call to dictionary\n    dumped = function_call.model_dump(exclude_none=True)",
        "function_call(\n            function_call=function_call,\n            chat_history=chat_history,\n            arguments=arguments,\n            function_call_count=function_call_count,\n            request_index=request_index,\n            function_behavior=function_call_behavior,\n        )",
        "function_call_or_result(msg)",
        "function_call_content import FunctionCallContent\nfrom semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent\nfrom semantic_kernel.exceptions import KernelFunctionAlreadyExistsError, KernelServiceNotFoundError\nfrom semantic_kernel.exceptions.content_exceptions import FunctionCallInvalidArgumentsException\nfrom semantic_kernel.exceptions.kernel_exceptions import (\n    KernelFunctionNotFoundError,\n    KernelInvokeException,\n    KernelPluginNotFoundError,\n    OperationCancelledException,\n)",
        "function_call_choice_configuration import FunctionCallChoiceConfiguration\n    from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n    from semantic_kernel.kernel import Kernel\n\n\nDEFAULT_MAX_AUTO_INVOKE_ATTEMPTS = 5\n\nlogger = logging.getLogger(__name__)",
        "function_call_contents\", return_value=None):\n        result = await AssistantThreadActions._handle_streaming_requires_action(\n            dummy_agent_name,\n            dummy_kernel,\n            dummy_run,\n            dummy_function_steps,  # type: ignore\n            dummy_args,\n        )",
        "function_call_with_continuation_on_malformed_arguments(kernel: Kernel, get_tool_call_mock)",
        "function_call_choice_config.available_functions:\n            logger.warning(\"No available functions. Skipping kernel function action group creation.\")",
        "function_call_arguments_delta\",\n        ),\n        param(\n            ResponseFunctionCallArgumentsDoneEvent(\n                call_id=\"call_id\",\n                arguments=\"argument delta\",\n                event_id=\"event_id\",\n                output_index=0,\n                item_id=\"item_id\",\n                response_id=\"response_id\",\n                type=\"response.function_call_arguments.done\",\n            )",
        "function_call_configuration_mistral(\n        self,\n        function_choice_configuration: \"FunctionCallChoiceConfiguration\",\n        settings: \"PromptExecutionSettings\",\n        type: \"FunctionChoiceType\",\n    )",
        "function_call_configuration_mistral sets tools etc.\"\"\"\n\n    chat_completion = MistralAIChatCompletion(\n        ai_model_id=\"test-model\",\n        api_key=\"test_key\",\n    )",
        "function_call in function_calls\n                ],\n            )\n\n            terminate_flag = any(result.terminate for result in results if result is not None)",
        "function_call_content(agent_name: str, fccs: list[FunctionCallContent])",
        "function_calling.functions_defined_in_json_prompt import (\n    main as function_defined_in_json_prompt,\n)",
        "function_calls = parse_return_control_payload(return_control_payload)",
        "function_call id=\"test\" name=\"func_name\">args</function_call></message>',\n            \"assistant\",\n            \"\",\n            1,\n        ),\n        (\n            '<message role=\"tool\"><function_result id=\"test\" name=\"func_name\">function result</function_result></message>',  # noqa: E501\n            \"tool\",\n            \"\",\n            1,\n        )",
        "function_call_contents.return_value = function_result_contents\n        agent.function_choice_behavior.maximum_auto_invoke_attempts = 2\n\n        mock_invoke_agent.side_effect = [\n            bedrock_agent_function_call_response,\n            bedrock_agent_streaming_simple_response,\n        ]\n        mock_start.return_value = \"test_session_id\"\n        async for _ in agent.invoke_stream(messages=\"test_input_text\")",
        "function_call_not_enough_parsed_args(kernel: Kernel, get_tool_call_mock)",
        "function_call_output\",\n                            \"output\": str(content.result)",
        "function_call_content import FunctionCallContent\nfrom semantic_kernel.contents.function_result_content import FunctionResultContent\nfrom semantic_kernel.contents.image_content import ImageContent\nfrom semantic_kernel.contents.text_content import TextContent\nfrom semantic_kernel.contents.utils.author_role import AuthorRole\n\nlogger: logging.Logger = logging.getLogger(__name__)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "crewai_tools": {
      "tool_name": "crewai_tools",
      "file_count": 46,
      "function_definitions": [
        "test_custom_llm",
        "test_agents_rpm_is_never_set_if_crew_max_RPM_is_not_set",
        "test_task_callback_on_crew",
        "test_crew_testing_function",
        "test_lite_agent_created_with_correct_parameters",
        "simple_agent",
        "test_tool_usage_finished_event_with_result",
        "reporting_task",
        "test_handle_streaming_tool_calls",
        "test_output_json_dict_hierarchical",
        "chat_loop",
        "test_stream_llm_emits_event_with_task_and_agent_info",
        "test_convert_to_model_with_invalid_json",
        "test_short_term_memory_save_events",
        "test_agent_use_trained_data",
        "test_hierarchical_kickoff_usage_metrics_include_manager",
        "validate_and_set_attributes",
        "test_handle_crew_planning_different_llm",
        "test_conditional_tasks_result_collection",
        "test_agent_execution"
      ],
      "class_definitions": [
        "MockCrew",
        "OpenAIAgentAdapter",
        "JWTAuthLLM",
        "ToolsHandler",
        "ScoreConverter",
        "ModelWithOptionalField",
        "LiteAgentOutput",
        "Address",
        "ReasoningFunction",
        "SimpleOutput",
        "AgentReasoningOutput",
        "CustomStorage",
        "FunctionCall",
        "LLM",
        "SimpleCrew",
        "LLMEventBase",
        "ModelWithDictField",
        "Agent",
        "ConverterError",
        "ScoreOutput"
      ],
      "import_dependencies": [
        "Optional",
        "execute_tool_and_check_finality",
        "LLMGuardrail",
        "pydantic_core",
        "trace",
        "repair_json",
        "InstructorToolCalling",
        "warnings",
        "ToolsHandler",
        "asyncio",
        "something",
        "ValidationError",
        "Agent, Crew, Task",
        "threading",
        "sleep",
        "create_crew",
        "crew",
        "Any, Dict, List, Optional, Union",
        "CrewPlanner",
        "Mock, patch"
      ],
      "function_calling_patterns": [
        "function_calling_llm: Optional[Union[str, InstanceOf[LLM], Any]] = Field(\n        description=\"Language model that will run the agent.\", default=None\n    )",
        "function_calling_llm=self.function_calling_llm,\n            respect_context_window=self.respect_context_window,\n            request_within_rpm_limit=(\n                self._rpm_controller.check_or_wait if self._rpm_controller else None\n            )",
        "function_calling_llm,\n            tool_name=tool.name,\n            attempts=self._run_attempts,\n        )\n        result = self._format_result(result=result)",
        "function_calling_llm\"] = llms[function_calling_llm]()",
        "function_calling_llm: Any = Field(default=None)",
        "function_calling_llm\", None)\n                                    else \"\"\n                                ),\n                                \"llm\": agent.llm.model,\n                                \"delegation_enabled?\": agent.allow_delegation,\n                                \"allow_code_execution?\": getattr(\n                                    agent, \"allow_code_execution\", False\n                                )",
        "function_calling_llm = None\n    agent.llm = Mock()",
        "function_calling_llm)\n                    error_message = self._i18n.errors(\"tool_usage_exception\")",
        "function_calling()",
        "function_calling_llm)\n                if self.task:\n                    self.task.increment_tools_errors()",
        "function_calling\") as supports_function_calling:\n        supports_function_calling.return_value = True\n        instructions = get_conversion_instructions(SimpleModel, llm)",
        "function_calling_llm: The language model that will run the tool calling for all the agents.\n        process: The process flow that the crew will follow (e.g., sequential, hierarchical)",
        "function_calling_llm or agent.llm\n    instructions = get_conversion_instructions(model, llm)",
        "function_calling_llm=MagicMock()",
        "function_calling.return_value = False\n    llm.call.return_value = \"Invalid JSON\"\n    sample_text = \"Name: Alice, Age: 30\"\n\n    instructions = get_conversion_instructions(SimpleModel, llm)",
        "function_calling_llm: Language model to be used for the tool usage.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools_handler: Optional[ToolsHandler],\n        tools: List[CrewStructuredTool],\n        task: Optional[Task],\n        function_calling_llm: Any,\n        agent: Optional[Union[\"BaseAgent\", \"LiteAgent\"]] = None,\n        action: Any = None,\n        fingerprint_context: Optional[Dict[str, str]] = None,\n    )",
        "function_calling(\n        self, tool_string: str\n    )",
        "function_calling\", return_value=False):\n        instructions = get_conversion_instructions(SimpleModel, llm)",
        "function_calling_llm=None,\n        agent=mock_agent,\n        action=MagicMock(tool=\"test_tool\")",
        "function_calling.return_value = False\n    llm.call.return_value = '{\"name\": \"Charlie\", \"age\": \"Not an age\"}'\n    sample_text = \"Charlie is thirty years old\"\n\n    instructions = get_conversion_instructions(SimpleModel, llm)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "haystack_tools": {
      "tool_name": "haystack_tools",
      "file_count": 46,
      "function_definitions": [
        "_serialize_usage",
        "test_from_component_with_inputs_from_state",
        "test_component_tool_in_pipeline_openai_tools_strict",
        "test_convert_streaming_chunk_to_chat_message_with_empty_tool_call_arguments",
        "test_run_with_custom_tool_parser",
        "faulty_tool_func",
        "test_resolve_hf_device_map_device_and_device_map",
        "test_run_with_tools_and_tool_response",
        "json_schema_github_compare",
        "test_from_assistant_with_tool_calls",
        "test_component_tool_serde",
        "test_from_dict_with_pre212_format_some_fields_missing",
        "test_init_tgi_invalid_url",
        "test_tool_breakpoint_in_pipeline_agent",
        "test_message_with_whitespace_handling",
        "test_from_openai_dict_format_assistant_message_with_content",
        "test_create_chunk_with_only_content",
        "test_from_user_with_content_parts_fails_unsupported_parts",
        "weather_tool",
        "test_complex_condition"
      ],
      "class_definitions": [
        "ComponentB",
        "HFEmbeddingAPIType",
        "SimpleComponent",
        "ToolCallDelta",
        "MockObject",
        "Address",
        "AnnotatedComponent",
        "TestComponentToolInPipeline",
        "ToolInvoker",
        "HFTokenStreamingHandler",
        "TestToolsetWithToolInvoker",
        "Agent",
        "MockChatGenerator",
        "TestHuggingFaceLocalChatGenerator",
        "AzureOpenAIChatGenerator",
        "TestJsonSchemaValidator",
        "ToolCall",
        "Callback",
        "ListProcessor",
        "TestToolInvokerUtilities"
      ],
      "import_dependencies": [
        "ThreadPoolExecutor",
        "StoppingCriteria, TextStreamer",
        "Optional",
        "AgentBreakpoint, AgentSnapshot, Breakpoint, ToolBreakpoint",
        "default_azure_ad_token_provider",
        "create_model",
        "SerperDevWebSearch",
        "ChatMessage, ImageContent, StreamingChunk, ToolCall",
        "Tool, Toolset",
        "asyncio",
        "StreamingCallbackT",
        "Pipeline, component, tracing",
        "ToolInvoker",
        "AsyncOpenAI, OpenAIError",
        "ListJoiner",
        "Pipeline as HfPipeline",
        "Mock, patch",
        "Component",
        "ChatMessage, ChatRole, StreamingChunk, ToolCall, ToolCallResult",
        "Enum"
      ],
      "function_calling_patterns": [
        "function_call\"\n        result = router.run(messages=[message], streams=[1, 2, 3], query=\"my query\")",
        "function_calling_schema(self, json_schema: dict[str, Any])",
        "function_call'}}\",\n                \"output\": \"{{streams}}\",\n                \"output_type\": list[int],\n                \"output_name\": \"streams\",\n            },\n            {\n                \"condition\": \"{{True}}\",\n                \"output\": \"{{query}}\",\n                \"output_type\": str,\n                \"output_name\": \"query\",\n            },  # catch-all condition\n        ]\n        router = ConditionalRouter(routes)",
        "function_calling_schema(json_schema)",
        "function_call\": \"tool_calls\",\n    }\n    # On very first chunk so len(previous_chunks)",
        "function_calling_schema(\n        self, json_schema_github_compare_openai, genuine_fc_message\n    )"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "llamaindex_tools": {
      "tool_name": "llamaindex_tools",
      "file_count": 175,
      "function_definitions": [
        "test_prepare_chat_with_tools_default_behavior",
        "test_astream_complete_when_j2__should_raise_error",
        "encode_documents",
        "test_invalid_handoff",
        "_prepare_chat_with_tools",
        "capture_event",
        "_gen",
        "test_prepare_chat_with_tools_explicit_tool_choice_overrides_tool_required",
        "test_anthropic_through_bedrock_async",
        "test_map_tool_choice_to_anthropic",
        "test_unsupported_version",
        "dashscope_api_response",
        "test_mixed_content_conversion",
        "test_completion",
        "mock_resolve_audio",
        "take_step",
        "test_zhipuai_llm_metadata",
        "empty_retriever_agent",
        "mock_completion_response",
        "test_process_response_event_with_text_annotation"
      ],
      "class_definitions": [
        "DummyTool",
        "StreamingAgentChatResponse",
        "InterruptionEvent",
        "Inner",
        "BaseAgentChatFormatter",
        "AsyncMockClient",
        "GuidanceQuestionGenerator",
        "SubQuestionAnswerPair",
        "TestUnsupportedOracleAdsVersionError",
        "TokenCounter",
        "Cortex",
        "NovitaAI",
        "MultiModalLLMMetadata",
        "OptionalContent",
        "system_prompt",
        "Address",
        "Schema",
        "ConversationInitEvent",
        "WandbCallbackHandler",
        "GmailOpenAIAgentPack"
      ],
      "import_dependencies": [
        "Message",
        "ChatCompletionMessageParam, ChatCompletionMessageToolCall",
        "BaseModel, ConfigDict, Field",
        "load_chat_store",
        "BaseQueryEngine",
        "TextBlock as LITextBlock",
        "DefaultAzureCredential",
        "Any, Optional, Sequence",
        "GuidancePydanticProgram",
        "MonkeyPatch",
        "TokenCounter",
        "FunctionCallingLLM, ToolSelection",
        "CodeGenerationModel",
        "StorageContext",
        "Field, field_validator",
        "FlexibleModel, create_flexible_model",
        "List, Optional",
        "Document, MediaResource, Node",
        "DispatcherSpanMixin",
        "RESPONSE_TYPE"
      ],
      "function_calling_patterns": [
        "FUNCTION_CALLING = {\n    \"o1-preview\",\n    \"o1-preview-2024-09-12\",\n    \"o1-mini\",\n    \"o1-mini-2024-09-12\",\n}\n\nGPT4_MODELS: Dict[str, int] = {\n    # stable model names:\n    #   resolves to gpt-4-0314 before 2023-06-27,\n    #   resolves to gpt-4-0613 after\n    \"gpt-4\": 8192,\n    \"gpt-4-32k\": 32768,\n    # turbo models (Turbo, JSON mode)",
        "function_calling_model()",
        "function_call_arguments.done\",\n        arguments='{\"arg\": \"value\"}',\n        sequence_number=1,\n    )\n\n    result = OpenAIResponses.process_response_event(\n        event=event,\n        tool_calls=updated_tool_calls,\n        built_in_tool_calls=built_in_tool_calls,\n        additional_kwargs=additional_kwargs,\n        current_tool_call=updated_call,\n        track_previous_responses=False,\n    )",
        "function_calling, api_key=api_key)\n    tool = FunctionTool.from_defaults(fn=get_current_time)",
        "function_calling,\n        [\"text\"],\n    )\n    assert chat_messages == azure_chat_messages_with_function_calling\n\n\ndef test_to_openai_tool_with_provided_description()",
        "function_call\": None,\n                                    \"audio\": None,\n                                },\n                            )()",
        "function_calling_model(model_name)",
        "function_calling_model: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        api_key = api_key or os.environ.get(\"GROQ_API_KEY\", None)",
        "FUNCTION_CALLING_OPTIONS\n\n\ndef create_retry_decorator(\n    max_retries: int,\n    min_seconds: float = 1,\n    max_seconds: float = 20,\n    random_exponential: bool = True,\n    stop_after_delay_seconds: Optional[float] = None,\n)",
        "function_call\"][\"arguments\"] = json.dumps(arguments)",
        "function_calling_config(mock_gemini_context: MagicMock)",
        "function_calling_model=self._client.is_function_calling_model(\n                self.model\n            )",
        "function_calling.additional_kwargs.get(key, None)",
        "function_call\" in additional_kwargs:\n                function_call = additional_kwargs.pop(\"function_call\")",
        "function_calling import FunctionCallingLLM\nfrom llama_index.core.llms.llm import ToolSelection, Model\nfrom llama_index.core.program.utils import process_streaming_objects, FlexibleModel\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.types import PydanticProgramMode\n\nif TYPE_CHECKING:\n    from llama_index.core.tools.types import BaseTool\n\nDEFAULT_REQUEST_TIMEOUT = 30.0\ndispatcher = get_dispatcher(__name__)",
        "function_calling_model,\n)\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.constants import DEFAULT_TEMPERATURE\nfrom llama_index.core.types import BaseOutputParser, PydanticProgramMode\nfrom tokenizers import Tokenizer\nfrom pydantic import ConfigDict, Field, PrivateAttr\nfrom openai import OpenAI as SyncOpenAI\nfrom openai import AsyncOpenAI\n\nDEFAULT_UPSTAGE_MODEL = \"solar-mini\"\n\nllm_retry_decorator = create_retry_decorator(\n    max_retries=6,\n    random_exponential=True,\n    stop_after_delay_seconds=60,\n    min_seconds=1,\n    max_seconds=20,\n)",
        "function_call_arguments.done\":\n            event = FunctionCallDoneEvent.model_validate(message)",
        "function_call is not None\n    assert result.parts[0].function_call.name == \"test_fn\"\n    assert result.parts[0].function_call.args == {\"arg1\": \"val1\"}\n\n\ndef test_convert_chat_message_to_gemini_content_with_content()",
        "function_calling_config(tool_required=True)",
        "function_calling_model: Optional[bool] = Field(\n        default=False,\n        # SEE: https://openai.com/blog/function-calling-and-other-api-updates\n        description=(\n            \"Set True if the model supports function calling messages, similar to\"\n            \" OpenAI's function calling API. For example, converting 'Email Anya to\"\n            \" see if she wants to get coffee next Friday' to a function call like\"\n            \" `send_email(to: string, body: string)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "anthropic_tools": {
      "tool_name": "anthropic_tools",
      "file_count": 6,
      "function_definitions": [],
      "class_definitions": [
        "TestAsyncMessages",
        "TestMessages"
      ],
      "import_dependencies": [
        "Anthropic, AsyncAnthropic",
        "ToolParam, MessageParam",
        "annotations",
        "assert_matches_type",
        "Any, cast",
        "Anthropic",
        "AsyncAnthropic",
        "pytest",
        "os",
        "DEPRECATED_MODELS",
        "asyncio"
      ],
      "function_calling_patterns": [],
      "api_endpoints": [],
      "configuration_files": []
    },
    "litellm_tools": {
      "tool_name": "litellm_tools",
      "file_count": 238,
      "function_definitions": [
        "test_router_provider_wildcard_routing",
        "test_completion_bedrock_httpx_command_r_sts_oidc_auth",
        "test_router_completion_vertex_exception",
        "test_vertexai_embedding",
        "test_get_tools_from_mcp_servers_handles_all_servers_failing",
        "completion_call",
        "test_zephyr_hf_tokens",
        "test_dalle_3_azure_cost_tracking",
        "test_get_user_agent_tags",
        "register_model",
        "test_multiple_function_call",
        "test_azure_instruct",
        "test_litellm_gateway_from_sdk_structured_output",
        "test_duration_in_seconds",
        "test_codellama_prompt_format",
        "test_url_context",
        "_get_span_context",
        "image_generation",
        "test_openai_transform_list_input_items_request_none_values",
        "test_cohere_embed_v4_error_handling"
      ],
      "class_definitions": [
        "Message",
        "BudgetRequest",
        "AnthropicConfig",
        "PrismaClient",
        "CohereV2ChatResponse",
        "TestTextFormatConversion",
        "TestOpenAIChatCompletion",
        "TestBedrockNovaJson",
        "TeamListResponseObject",
        "ClientSideFallbackModel",
        "MockContentGuardrail",
        "ImageAttributes",
        "Step",
        "TestAzureOpenAIO3",
        "OpenAIRealtimeConversationCreated",
        "OrganizationMemberUpdateResponse",
        "DecodedResponseId",
        "CustomRoutingStrategyBase",
        "ToolCalls",
        "MockProxyLogging"
      ],
      "import_dependencies": [
        "",
        "DatabricksBase, DatabricksException",
        "Message",
        "get_team_object",
        "remove_items_at_indices",
        "HuggingFaceEmbeddingConfig",
        "Rules",
        "make_pipeline",
        "litellm.llms.gemini",
        "timedelta",
        "check_response_size_is_safe",
        "map_finish_reason, process_response_headers",
        "verbose_logger, verbose_router_logger",
        "Iterable, List, Optional, Union",
        "GenerateContentToCompletionHandler",
        "router as google_router",
        "ProxyLogging, hash_token",
        "BedrockInvokeNovaRequest",
        "LITELLM_METADATA_FIELD, OLD_LITELLM_METADATA_FIELD",
        "CustomStreamWrapper, ModelResponse, Usage"
      ],
      "function_calling_patterns": [
        "function_calls = []\n        \n        chunk_count = 0\n        async for chunk in response:\n            chunk_count += 1\n            print(f\"Received chunk {chunk_count}: {chunk}\")",
        "function_call_to_chat_completion_message(\n                function_call=input_item\n            )",
        "function_call = True\n            cohere_tools = self._construct_cohere_tool(tools=optional_params[\"tools\"])",
        "function_call: Optional[Union[str, dict]] = None,\n        functions: Optional[list] = None,\n        logit_bias: Optional[dict] = None,\n        max_tokens: Optional[int] = None,\n        n: Optional[int] = None,\n        presence_penalty: Optional[int] = None,\n        stop: Optional[Union[str, list]] = None,\n        temperature: Optional[int] = None,\n        top_p: Optional[int] = None,\n    ) -> None:\n        locals_ = locals()",
        "function_call\"] = functions\n\n            if thinking_blocks is not None:\n                chat_completion_message[\"thinking_blocks\"] = thinking_blocks  # type: ignore\n\n            if isinstance(model_response, ModelResponseStream)",
        "function_call (str, optional)",
        "function_call = json.loads(response_json_message[\"content\"])",
        "function_call: Optional[Union[str, Dict[str, Any]]] = None\n    functions: Optional[List[Dict[str, Any]]] = None\n    user: Optional[str] = None\n    stream: Optional[bool] = None\n\n    # LiteLLM-specific metadata param (from original ChatCompletionRequest)",
        "function_call\",\n            \"functions\",\n            \"logit_bias\",\n            \"max_tokens\",\n            \"max_completion_tokens\",\n            \"n\",\n            \"presence_penalty\",\n            \"stop\",\n            \"temperature\",\n            \"top_p\",\n            \"response_format\",\n            \"tools\",\n            \"tool_choice\"\n        ]\n\n        if litellm.supports_reasoning(\n            model=model,\n            custom_llm_provider=self.custom_llm_provider,\n        )",
        "function_call=None,\n                    tool_calls=None,\n                    audio=None,\n                ),\n                logprobs=None,\n            )\n        ],\n        provider_specific_fields=None,\n        stream_options={\"include_usage\": True},\n        usage=Usage(\n            completion_tokens=5,\n            prompt_tokens=11779,\n            total_tokens=11784,\n            completion_tokens_details=None,\n            prompt_tokens_details=PromptTokensDetails(\n                audio_tokens=None, cached_tokens=11775\n            )",
        "function_call_output_item)\n        assert not LiteLLMCompletionResponsesConfig._is_input_item_tool_call_output(function_call_item)",
        "function_call_arguments()",
        "function_call=None,\n                tool_calls=[\n                    ChatCompletionDeltaToolCall(\n                        id=None,\n                        function=Function(arguments=': \"San ', name=None)",
        "function_call\": None,\n                        \"tool_calls\": None,\n                    },\n                }\n            ],\n            \"created\": 1742856796,\n            \"model\": \"gpt-4o-2024-08-06\",\n            \"object\": \"chat.completion\",\n            \"service_tier\": \"default\",\n            \"system_fingerprint\": \"fp_6ec83003ad\",\n            \"usage\": {\n                \"completion_tokens\": 10,\n                \"prompt_tokens\": 9,\n                \"total_tokens\": 19,\n                \"completion_tokens_details\": {\n                    \"accepted_prediction_tokens\": 0,\n                    \"audio_tokens\": 0,\n                    \"reasoning_tokens\": 0,\n                    \"rejected_prediction_tokens\": 0,\n                },\n                \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0},\n            },\n        }\n    )\n\n    with patch.object(\n        openai_client.chat.completions.with_raw_response,\n        \"create\",\n        return_value=mock_response,\n    )",
        "function_call_example = {\n    \"id\": \"chatcmpl-7zVNA4sXUftpIg6W8WlntCyeBj2JY\",\n    \"object\": \"chat.completion\",\n    \"created\": 1694892960,\n    \"model\": \"gpt-3.5-turbo\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": None,\n                \"function_call\": {\n                    \"name\": \"get_current_weather\",\n                    \"arguments\": '{\\n  \"location\": \"Boston, MA\"\\n}',\n                },\n            },\n            \"finish_reason\": \"function_call\",\n        }\n    ],\n    \"usage\": {\"prompt_tokens\": 82, \"completion_tokens\": 18, \"total_tokens\": 100},\n}\n\nfunction_calling_output_structure = {\n    \"id\": str,\n    \"object\": str,\n    \"created\": int,\n    \"model\": str,\n    \"choices\": [\n        {\n            \"index\": int,\n            \"message\": {\n                \"role\": str,\n                \"content\": (type(None)",
        "function_call(self, logging_obj)",
        "function_call\") is not None\n                ):  # support assistant tool invoke conversion\n                    gemini_tool_call_parts = convert_to_gemini_tool_call_invoke(\n                        assistant_msg\n                    )",
        "function_calling to return False\n        with patch('litellm.supports_function_calling', return_value=False)",
        "function_call in function_calls:\n                follow_up_input.append(function_call)",
        "function_calling_support(self, proxy_model, expected_result)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    },
    "instructor_tools": {
      "tool_name": "instructor_tools",
      "file_count": 36,
      "function_definitions": [
        "test_extract_from_codeblock_no_language",
        "extract",
        "disable_pydantic_error_url",
        "remove_control_chars",
        "test_validate_model_non_strict",
        "test_extract_from_stream",
        "test_validate_model_json_error",
        "test_extract_text_simple_format",
        "test_generate_anthropic_schema_matches_class_method",
        "test_validate_model_strict",
        "process_potential_object",
        "test_invalid_json",
        "my_property",
        "test_generate_gemini_schema_matches_class_method",
        "generate",
        "get_spans",
        "test_refusal_attribute",
        "test_no_refusal_attribute",
        "test_field_descriptions",
        "validateIsSubClass"
      ],
      "class_definitions": [
        "SystemMessage",
        "UnknownFormat",
        "MakeFieldsOptional",
        "Partial",
        "GoogleSearch",
        "Multiply",
        "Table",
        "Dataframe",
        "Weather",
        "VertexAIParallelBase",
        "BatchJob",
        "MockCompletion",
        "Query",
        "Instructions",
        "UserDetail",
        "AdapterBase",
        "PartialBase",
        "Diff",
        "TestModel",
        "Question"
      ],
      "import_dependencies": [
        "Image, Audio",
        "from_writer",
        "Optional",
        "Message",
        "vertexai_process_response",
        "ModeError",
        "warnings",
        "traceable",
        "asyncio",
        "AdapterBase",
        "PartialBase",
        "Usage as AnthropicUsage",
        "from_xai",
        "OpenAISchema, openai_schema",
        "CompletionUsage as OpenAIUsage",
        "ValidationError",
        "pandas as pd",
        "AsyncInstructor, Instructor",
        "Any, Union, get_origin",
        "typing"
      ],
      "function_calling_patterns": [
        "function_calls import openai_schema\n\n    the_types = get_types_array(typehint)",
        "function_call = openai_schema(base_model)",
        "function_call.arguments:\n                            yield json_chunk\n                    elif mode in {\n                        Mode.JSON,\n                        Mode.MD_JSON,\n                        Mode.JSON_SCHEMA,\n                        Mode.CEREBRAS_JSON,\n                        Mode.FIREWORKS_JSON,\n                        Mode.PERPLEXITY_JSON,\n                        Mode.WRITER_JSON,\n                    }:\n                        if json_chunk := chunk.choices[0].delta.content:\n                            yield json_chunk\n                    elif mode in {\n                        Mode.TOOLS,\n                        Mode.TOOLS_STRICT,\n                        Mode.FIREWORKS_TOOLS,\n                        Mode.WRITER_TOOLS,\n                    }:\n                        if json_chunk := chunk.choices[0].delta.tool_calls:\n                            if json_chunk[0].function.arguments:\n                                yield json_chunk[0].function.arguments\n                    else:\n                        raise NotImplementedError(\n                            f\"Mode {mode} is not supported for MultiTask streaming\"\n                        )",
        "function_calls import OpenAISchema, openai_schema\nfrom .multimodal import convert_messages\nfrom .response import (\n    handle_response_model,\n    process_response,\n    process_response_async,\n    handle_reask_kwargs,\n)",
        "function_call={\"name\": Program.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code. with correct imports.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": data,\n            },\n        ],\n        max_tokens=1000,\n    )\n    return Program.from_response(completion)",
        "function_call\"])\n    return ret\n\n\ndef is_async(func: Callable[..., Any])",
        "function_call\": {\n                    \"name\": \"Multiply\",\n                    \"arguments\": '{\\n  \"a\": 169,\\n  \"b\": 166,\\n  \"result\": 28054\\n}',\n                },\n            },\n        ],\n        \"functions\": [\n            {\n                \"name\": \"Multiply\",\n                \"description\": \"Correctly extracted `Multiply` with all the required parameters with correct types\",\n                \"parameters\": {\n                    \"properties\": {\n                        \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n                        \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n                        \"result\": {\n                            \"description\": \"The result of the multiplication\",\n                            \"title\": \"Result\",\n                            \"type\": \"integer\",\n                        },\n                    },\n                    \"required\": [\"a\", \"b\", \"result\"],\n                    \"type\": \"object\",\n                },\n            }\n        ],\n    }\n    for _ in range(10)",
        "function_call\": {\"name\": \"TestModel\", \"arguments\": data_content},\n                \"content\": data_content,\n            },\n            \"finish_reason\": finish_reason,\n        }\n    ]\n\n    completion = ChatCompletion(\n        id=\"test_id\",\n        choices=mock_choices,\n        created=1234567890,\n        model=\"gpt-3.5-turbo\",\n        object=\"chat.completion\",\n    )",
        "function_call={\"name\": Diff.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of refactor \"\n                \"existing python repositories. You will name files correct, include \"\n                \"__init__.py files and write correct python code, with correct imports. \"\n                \"You'll deliver your changes in valid 'diff' format so that they could \"\n                \"be applied using the 'patch' command. \"\n                \"Make sure you put the correct line numbers, \"\n                \"and that all lines that must be changed are correctly marked.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": new_requirements,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": program_description,\n            },\n        ],\n        max_tokens=1000,\n    )\n    return Diff.from_response(completion)",
        "function_call\"] = {\n        \"name\": generate_openai_schema(response_model)",
        "function_calls import OpenAISchema\n\n        assert is_simple_type(response_model)",
        "function_call = response.candidates[0].content.parts[0].function_call\n    kwargs[\"contents\"].append(\n        types.ModelContent(\n            parts=[\n                types.Part.from_function_call(\n                    name=function_call.name,\n                    args=function_call.args,\n                )",
        "function_calls import OpenAISchema, openai_schema\n\n    if not issubclass(response_model, OpenAISchema)",
        "function_calling_config=ToolConfig.FunctionCallingConfig(\n            mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n        )",
        "function_call.name,\n                    args=response.parts[0].function_call.args,\n                )\n            ],\n        },\n        {\n            \"role\": \"function\",\n            \"parts\": [\n                glm.Part(\n                    function_response=glm.FunctionResponse(\n                        name=response.parts[0].function_call.name,\n                        response={\"error\": f\"Validation Error(s)",
        "function_call\")\n        and message.function_call is not None\n        and ret[\"content\"]\n    ):\n        if not isinstance(ret[\"content\"], str)",
        "function_calls import OpenAISchema, openai_schema\nfrom .processing.schema import (\n    generate_openai_schema,\n    generate_anthropic_schema,\n    generate_gemini_schema,\n)",
        "function_call.name,\n                response={\n                    \"content\": f\"Validation Error found:\\n{exception}\\nRecall the function correctly, fix the errors\"\n                },\n            )\n        ]\n    )\n\n\ndef vertexai_process_response(\n    _kwargs: dict[str, Any],\n    model: Union[BaseModel, list[BaseModel], type],  # noqa: UP007\n)",
        "function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Function name does not match\"\n        return cls.model_validate_json(\n            message.function_call.arguments,  # type: ignore[attr-defined]\n            context=validation_context,\n            strict=strict,\n        )",
        "function_call\n                    resp_dict = type(resp)"
      ],
      "api_endpoints": [],
      "configuration_files": []
    }
  },
  "common_patterns": [
    "Common functions: {'multiply': 1, 'add': 1, 'addition': 1, 'prompt': 1, 'failing_dynamic_model': 1}",
    "Common classes: {'Address': 4, 'Agent': 3, 'Message': 3, 'Step': 3, 'ToolCalls': 2}",
    "Common imports: {'Message': 5, 'ValidationError': 4, 'asyncio': 4, 'List, Optional': 3, 'pandas as pd': 3, 'Optional': 3, 'sleep': 2, 'CancellationToken': 2, 'DefaultAzureCredential': 2, 'AsyncGenerator': 2}"
  ],
  "hosting_recommendations": {
    "langchain_tools": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "openai_function_calling": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "autogen_tools": {
      "deployment_method": "containerized_service",
      "server_requirements": "Python 3.8+, framework dependencies",
      "hosting_options": [
        "docker_compose",
        "kubernetes",
        "standalone_server"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "semantic_kernel_skills": {
      "deployment_method": "multi_runtime_container",
      "server_requirements": ".NET Core + Python, hybrid runtime",
      "hosting_options": [
        "docker_multi_stage",
        "kubernetes_multi_container"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "crewai_tools": {
      "deployment_method": "containerized_service",
      "server_requirements": "Python 3.8+, framework dependencies",
      "hosting_options": [
        "docker_compose",
        "kubernetes",
        "standalone_server"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "haystack_tools": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "llamaindex_tools": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "anthropic_tools": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    },
    "litellm_tools": {
      "deployment_method": "custom_solution",
      "server_requirements": "Analysis needed",
      "hosting_options": [
        "custom_deployment"
      ],
      "docker_feasible": false,
      "api_wrapper_needed": true
    },
    "instructor_tools": {
      "deployment_method": "pip_package",
      "server_requirements": "Python 3.8+, minimal dependencies",
      "hosting_options": [
        "PyPI",
        "private_pypi",
        "git_repository"
      ],
      "docker_feasible": true,
      "api_wrapper_needed": true
    }
  },
  "deployment_complexity": {}
}